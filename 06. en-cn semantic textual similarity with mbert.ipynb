{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the procedure in other notebook to import dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from pprint import pprint\n",
    "from sentence_transformers import util\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Define the paths to the dataset files\n",
    "english_dataset_path = r'./dataset/News-Commentary/News-Commentary.en-zh.en'\n",
    "chinese_dataset_path = r'./dataset/News-Commentary/News-Commentary.en-zh.zh'\n",
    "\n",
    "# Function to get random sentence pairs\n",
    "def get_random_sentence_pairs(english_path, chinese_path, num_pairs=1):\n",
    "    with open(english_path, 'r', encoding='utf-8') as eng_file, \\\n",
    "         open(chinese_path, 'r', encoding='utf-8') as zh_file:\n",
    "        \n",
    "        # Read all lines from both files\n",
    "        english_lines = eng_file.readlines()\n",
    "        chinese_lines = zh_file.readlines()\n",
    "        \n",
    "        # Ensure both files have the same number of lines\n",
    "        if len(english_lines) != len(chinese_lines):\n",
    "            print(\"Error: The files don't have the same number of lines.\")\n",
    "            return None\n",
    "\n",
    "        # Generate random indices\n",
    "        random_indices = random.sample(range(len(english_lines)), num_pairs)\n",
    "        \n",
    "        # Get the random sentence pairs\n",
    "        sentence_pairs = [(english_lines[index].strip(), chinese_lines[index].strip()) for index in random_indices]\n",
    "        \n",
    "        return sentence_pairs\n",
    "\n",
    "# Function to perform mean pooling on the model outputs\n",
    "import torch\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Function to compute and print the cosine similarity scores\n",
    "# Function to compute and print the cosine similarity scores using multilingual BERT\n",
    "def print_similarity_scores(sentence_pairs):\n",
    "    \n",
    "    # Initialize the tokenizer and model for multilingual BERT\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    # Separate the sentence pairs\n",
    "    sentences1, sentences2 = zip(*sentence_pairs)\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    encoded_input1 = tokenizer(sentences1, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input2 = tokenizer(sentences2, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute embeddings for both lists\n",
    "    with torch.no_grad():\n",
    "        model_output1 = model(**encoded_input1)\n",
    "        model_output2 = model(**encoded_input2)\n",
    "    \n",
    "    # Mean pooling to get one vector per sentence\n",
    "    embeddings1 = mean_pooling(model_output1, encoded_input1['attention_mask'])\n",
    "    embeddings2 = mean_pooling(model_output2, encoded_input2['attention_mask'])\n",
    "\n",
    "    # Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # Output the pairs with their score\n",
    "    for i in range(len(sentences1)):\n",
    "        print(f\"{sentences1[i]} \\n {sentences2[i]} \\n Score: {cosine_scores[i][i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its member states are constantly being evaluated for their economic potential and desirability as a market for investments, goods, and services. At the same time, their effort to forge a community free from external intervention is shaping a new regional order based on common security and shared prosperity. \n",
      " 发自雅加达 — — 东南亚联盟如今正迎来一个决定性的时刻。 一直以来，全世界都在评估东盟各成员国作为一个整体性投资、商品和服务市场的潜力和可行性。 而与此同时，东盟各国也正在努力构建一个不受外部干涉的联盟，并以此形成一个基于共同安全和共同繁荣的新秩序。 \n",
      " Score: 0.7592\n",
      "Today, whether owing to “neo-Ottomanism” or just old-fashioned nasty governance under a thin-skinned autocrat (President Recep Tayyip Erdoğan) with Islamist tendencies, Turkey is on many watch lists. Its intervention in Middle East politics has been furtive and lacking in consistency or clarity. \n",
      " 今天，无论是因为“新奥特曼主义”抑或仅仅是有伊斯兰倾向的小气的独裁者老掉牙的令人讨厌的治理（总统雷杰普·塔伊普·埃尔多安 ） ， 土耳其已经进入到不止一份观察名单。 土耳其一直偷偷摸摸地干预中东政治，这种干预既缺乏一致性条理也不甚清晰。 \n",
      " Score: 0.7575\n",
      "The conventional wisdom is, however, wrong; worse, it is dangerous, for we have all seen how quickly it can take hold, distort reality, and then harden like cement. For example, four years ago, when I came to office, only a handful of global leaders knew enough even to talk about climate change – the defining challenge of our times, whose effects we see every day, all around us. \n",
      " 但多数人的看法不仅错误，而且危险，因为我们都知道错误的看法能快速生根，扭曲事实，而后像水泥一样固化。 比方说四年前，我刚刚就任的时候，多数全球领导人谈起气候变化这个时代议题时都知之甚少，但现在气候变化的影响已经深入人心、随处可见。 今天，气候变化已经成为全球首要议题。 \n",
      " Score: 0.6843\n",
      "The collective response, adopted in 2000, has been the “Lisbon Strategy,” which aims at making the EU “the most dynamic and competitive knowledge-based economy in the world” by 2010. More realistically, it is designed to provide governments with the incentive to undertake the reforms that stimulate economic growth and productivity. \n",
      " 这主要是由于欧盟三大国的问题，即法国、德国和意大利。 三国的国民生产总值占欧洲的70 ％ 。 2000年采取的联合举动是所谓的“里斯本战略 ” ， 其目标是将欧盟在2010年前变成“世界上最富活力和竞争力的知识经济体 ” 。 出于更为现实的原因，这一战略激励各国政府实施改革，解决抑制经济增长和生产效率的问题。 \n",
      " Score: 0.7497\n",
      "The ECB has been following a similar strategy of large-scale asset purchases and extremely low (indeed negative) short-term interest rates. But, although the policy is the same as the Fed’s, its purpose is very different. \n",
      " 欧洲央行效仿了类似的大规模买入资产和极低（实际为负）的短期利率策略。 但是，尽管政策和美联储一样，其目的却大不相同。 \n",
      " Score: 0.7175\n"
     ]
    }
   ],
   "source": [
    "# Load the SentenceTransformer model\n",
    "\n",
    "# Example usage:\n",
    "# Generate 5 random sentence pairs\n",
    "num_of_gen = 5\n",
    "sentence_pair = get_random_sentence_pairs(english_dataset_path, chinese_dataset_path, num_of_gen)\n",
    "\n",
    "# Compute and print the similarity scores for the sentence pairs\n",
    "if sentence_pair:\n",
    "    print_similarity_scores(sentence_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
