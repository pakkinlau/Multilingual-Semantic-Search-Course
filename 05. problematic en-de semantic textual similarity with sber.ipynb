{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the procedure in other notebook to import dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from pprint import pprint\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Define the paths to the dataset files\n",
    "english_dataset_path = r'./dataset2/News-Commentary/News-Commentary.de-en.en'\n",
    "german_dataset_path = r'./dataset2/News-Commentary/News-Commentary.de-en.de'\n",
    "\n",
    "# Function to get random sentence pairs\n",
    "def get_random_sentence_pairs(english_path, chinese_path, num_pairs=1):\n",
    "    with open(english_path, 'r', encoding='utf-8') as eng_file, \\\n",
    "         open(chinese_path, 'r', encoding='utf-8') as zh_file:\n",
    "        \n",
    "        # Read all lines from both files\n",
    "        english_lines = eng_file.readlines()\n",
    "        chinese_lines = zh_file.readlines()\n",
    "        \n",
    "        # Ensure both files have the same number of lines\n",
    "        if len(english_lines) != len(chinese_lines):\n",
    "            print(\"Error: The files don't have the same number of lines.\")\n",
    "            return None\n",
    "\n",
    "        # Generate random indices\n",
    "        random_indices = random.sample(range(len(english_lines)), num_pairs)\n",
    "        \n",
    "        # Get the random sentence pairs\n",
    "        sentence_pairs = [(english_lines[index].strip(), chinese_lines[index].strip()) for index in random_indices]\n",
    "        \n",
    "        return sentence_pairs\n",
    "\n",
    "# Function to compute and print the cosine similarity scores\n",
    "def print_similarity_scores(sentence_pairs):\n",
    "    \n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    # Separate the sentence pairs\n",
    "    sentences1, sentences2 = zip(*sentence_pairs)\n",
    "\n",
    "    # Compute embedding for both lists\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # Output the pairs with their score\n",
    "    for i in range(len(sentences1)):\n",
    "        print(f\"{sentences1[i]} \\n {sentences2[i]} \\n Score: {cosine_scores[i][i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The European Commission has played a very helpful role through its structural funds, which cover up to 80% of the additional costs involved in integrating the Roma. \n",
      " Die Europäische Kommission hat durch ihre Strukturfonds, die bis zu 80% der bei der Integration der Roma anfallenden zusätzlichen Kosten abdecken, eine hilfreiche Rolle gespielt. \n",
      " Score: 0.5350\n",
      "But it is not an easy matter to assess how the time savings implied by online shopping, or the cost savings that might result from increased competition (owing to greater ease of price comparison online), affects our standard of living. \n",
      " Aber es ist keine einfache Sache, einzuschätzen, wie die vom Online-Einkauf ausgehende Zeitersparnis oder die möglichen Kosteneinsparungen, die von einem verstärkten Wettbewerb herrühren (bedingt durch leichtere Online-Preisvergleiche), unseren Lebensstandard beeinflussen. \n",
      " Score: 0.2289\n",
      "According to most analysts, the novelty in the announcement was the Fed’s willingness to be explicit about its quantitative policy thresholds and, therefore, about the future course of its monetary policy. \n",
      " Außerdem ließ die Fed keinen Zweifel daran, dieses Vorhaben beschleunigen zu wollen, bis die Arbeitslosenrate in den USA erheblich – auf mindestens 6,5 Prozent - sinkt und sich die Inflation auf einen Wert von 2,5 Prozent oder darunter einpendelt. \n",
      " Score: 0.2959\n",
      "None is likely in China in the near future. \n",
      " Doch seit damals sank der Anteil des Renminbi im internationalen Zahlungsverkehr. \n",
      " Score: 0.0155\n",
      "But as he exits his post, there is a growing consensus that history will record the straight-talking Dutchman as being the right man for the right job at the right time. \n",
      " Doch nun, da er seinen Posten verlässt, gibt es eine wachsende Übereinstimmung darin, dass die Geschichte über den geradeheraus sprechenden Holländer urteilen wird, er war der rechte Mann für die richtige Aufgabe zum richtigen Zeitpunkt. \n",
      " Score: 0.1754\n"
     ]
    }
   ],
   "source": [
    "# Load the SentenceTransformer model\n",
    "\n",
    "# Example usage:\n",
    "# Generate 5 random sentence pairs\n",
    "num_of_gen = 5\n",
    "sentence_pair = get_random_sentence_pairs(english_dataset_path, german_dataset_path, num_of_gen)\n",
    "\n",
    "# Compute and print the similarity scores for the sentence pairs\n",
    "if sentence_pair:\n",
    "    print_similarity_scores(sentence_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The similarity score is surprisingly low.\n",
    "\n",
    "Reasons:\n",
    "- 1. Cross-lingual Embeddings Challenge: However, cross-lingual semantic similarity can be particularly challenging, especially when dealing with languages that are structurally different, like English and Chinese. These languages not only differ in grammar and syntax but also in cultural context and expression.\n",
    "- 2. Literal vs Contextual Meaning: The model may be capturing the literal translation of words rather than the contextual meaning of the whole sentence. This can lead to lower scores if the sentence structure or idiomatic expressions vary significantly between the two languages.\n",
    "- 3. Quality of the Model: While SentenceTransformer models are generally quite good, their performance on specific language pairs can vary. There might be better-suited models for English-Chinese similarity specifically, such as those trained on cross-lingual datasets.\n",
    "- 4. Model's Training Data: If the model hasn't been trained on a diverse enough dataset that includes similar sentence pairs to those you're comparing, its ability to accurately measure similarity across languages may be limited.\n",
    "- 5. Embedding Space Alignment: For cross-lingual similarity, the embedding spaces of both languages must be well-aligned. If the alignment is not accurate, even semantically similar sentences might end up far apart in the embedding space, resulting in low similarity scores.\n",
    "- 6. Complex Sentences: The examples given include relatively complex sentences with multiple clauses and potentially nuanced meanings. The model might struggle more with these than with simpler, more straightforward sentences.\n",
    "- 7. Possible Errors: Ensure there are no errors in data preprocessing, such as incorrect sentence pairings or issues with text encoding that could affect the embeddings generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
