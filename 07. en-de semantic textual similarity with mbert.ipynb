{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the procedure in other notebook to import dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from pprint import pprint\n",
    "from sentence_transformers import util\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Define the paths to the dataset files\n",
    "english_dataset_path = r'./dataset2/News-Commentary/News-Commentary.de-en.en'\n",
    "german_dataset_path = r'./dataset2/News-Commentary/News-Commentary.de-en.de'\n",
    "\n",
    "# Function to get random sentence pairs\n",
    "def get_random_sentence_pairs(english_path, chinese_path, num_pairs=1):\n",
    "    with open(english_path, 'r', encoding='utf-8') as eng_file, \\\n",
    "         open(chinese_path, 'r', encoding='utf-8') as zh_file:\n",
    "        \n",
    "        # Read all lines from both files\n",
    "        english_lines = eng_file.readlines()\n",
    "        chinese_lines = zh_file.readlines()\n",
    "        \n",
    "        # Ensure both files have the same number of lines\n",
    "        if len(english_lines) != len(chinese_lines):\n",
    "            print(\"Error: The files don't have the same number of lines.\")\n",
    "            return None\n",
    "\n",
    "        # Generate random indices\n",
    "        random_indices = random.sample(range(len(english_lines)), num_pairs)\n",
    "        \n",
    "        # Get the random sentence pairs\n",
    "        sentence_pairs = [(english_lines[index].strip(), chinese_lines[index].strip()) for index in random_indices]\n",
    "        \n",
    "        return sentence_pairs\n",
    "\n",
    "# Function to perform mean pooling on the model outputs\n",
    "import torch\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Function to compute and print the cosine similarity scores\n",
    "# Function to compute and print the cosine similarity scores using multilingual BERT\n",
    "def print_similarity_scores(sentence_pairs):\n",
    "    \n",
    "    # Initialize the tokenizer and model for multilingual BERT\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    # Separate the sentence pairs\n",
    "    sentences1, sentences2 = zip(*sentence_pairs)\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    encoded_input1 = tokenizer(sentences1, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input2 = tokenizer(sentences2, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute embeddings for both lists\n",
    "    with torch.no_grad():\n",
    "        model_output1 = model(**encoded_input1)\n",
    "        model_output2 = model(**encoded_input2)\n",
    "    \n",
    "    # Mean pooling to get one vector per sentence\n",
    "    embeddings1 = mean_pooling(model_output1, encoded_input1['attention_mask'])\n",
    "    embeddings2 = mean_pooling(model_output2, encoded_input2['attention_mask'])\n",
    "\n",
    "    # Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # Output the pairs with their score\n",
    "    for i in range(len(sentences1)):\n",
    "        print(f\"{sentences1[i]} \\n {sentences2[i]} \\n Score: {cosine_scores[i][i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A declaration of independence by Kosovo will likely bring a similar declaration from Georgia’s breakaway Abkhazia region, which Russia could well recognize. \n",
      " Sollte Georgien militärische Schritte unternehmen, um das zu verhindern, würde Russlands Militär wahrscheinlich mit Gewalt reagieren und dabei eine Situation schaffen, die außer Kontrolle geraten könnte. \n",
      " Score: 0.6652\n",
      "You cannot divert the trolley. \n",
      " Der Waggon kann nicht mehr umgeleitet werden. \n",
      " Score: 0.5164\n",
      "But a lump-sum tax on robots would merely lead robot producers to bundle artificial intelligence within other machinery. \n",
      " Doch eine auf Roboter erhobene Pauschalsteuer würde lediglich dazu führen, dass die Roboterhersteller künstliche Intelligenz mit anderen Maschinen bündeln würden. \n",
      " Score: 0.6602\n",
      "Many challenges confront India on its path to sustained strong growth, principally that of converting the country’s vast promise into reality. \n",
      " Auf seinem Weg in Richtung nachhaltiges starkes Wachstum ist Indien mit zahlreichen Herausforderungen konfrontiert, vor allem, wenn es darum geht, das große Versprechen des Landes in die Realität umzusetzen. \n",
      " Score: 0.6880\n",
      "The Promise of Liberal Identity Politics \n",
      " Das Versprechen liberaler Identitätspolitik \n",
      " Score: 0.6414\n"
     ]
    }
   ],
   "source": [
    "# Load the SentenceTransformer model\n",
    "\n",
    "# Example usage:\n",
    "# Generate 5 random sentence pairs\n",
    "num_of_gen = 5\n",
    "sentence_pair = get_random_sentence_pairs(english_dataset_path, german_dataset_path, num_of_gen)\n",
    "\n",
    "# Compute and print the similarity scores for the sentence pairs\n",
    "if sentence_pair:\n",
    "    print_similarity_scores(sentence_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
